


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>neural_compressor.experimental.pytorch_pruner.patterns &mdash; Intel® Neural Compressor  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 

  
  <script src="../../../../../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://shiftlab.github.io/pytorch/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://shiftlab.github.io/pytorch/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/features">Features</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

  
<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <div class="navbar-logo">
          <a href="https://intel.github.io/neural-compressor/Welcome.html" alt="Intel homepage" class="intel-logo-rebrand">
            <span class="headTitleStyle"> Intel® Neural Compressor</span> 
          </a>
        <div class="version">
              <a href="../../../../../../versions.html">2.0▼</a>
              <p>Click link above to switch version</p>
            </div>
    </div>
      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  <div>

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
   

          </div>

          

            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Welcome.html">Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">

      <section data-toggle="wy-nav-shift" class="pytorch-content-wrap">
        <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
          <div class="pytorch-breadcrumbs-wrapper">
            















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li><code class="xref py py-mod docutils literal notranslate"><span class="pre">neural_compressor.experimental.pytorch_pruner.patterns</span></code></li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../../../../../_sources/autoapi/neural_compressor/experimental/pytorch_pruner/patterns/index.rst.txt" rel="nofollow"><img src="../../../../../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
          </div>

          <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
            Shortcuts
          </div>
        </div>

        <div class="pytorch-content-left">
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" class="pytorch-article">
              
  <section id="module-neural_compressor.experimental.pytorch_pruner.patterns">
<span id="neural-compressor-experimental-pytorch-pruner-patterns"></span><h1><a class="reference internal" href="#module-neural_compressor.experimental.pytorch_pruner.patterns" title="neural_compressor.experimental.pytorch_pruner.patterns"><code class="xref py py-mod docutils literal notranslate"><span class="pre">neural_compressor.experimental.pytorch_pruner.patterns</span></code></a><a class="headerlink" href="#module-neural_compressor.experimental.pytorch_pruner.patterns" title="Permalink to this heading">¶</a></h1>
<p>pattern module.</p>
<section id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this heading">¶</a></h2>
<section id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern" title="neural_compressor.experimental.pytorch_pruner.patterns.Pattern"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Pattern</span></code></a></p></td>
<td><p>Pruning Pattern.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM" title="neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PatternNxM</span></code></a></p></td>
<td><p>Pruning Pattern.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM" title="neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PatternNInM</span></code></a></p></td>
<td><p>Pruning Pattern.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.register_pattern" title="neural_compressor.experimental.pytorch_pruner.patterns.register_pattern"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_pattern</span></code></a>(name)</p></td>
<td><p>Class decorator used to register a Pattern subclass to the registry.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.get_pattern" title="neural_compressor.experimental.pytorch_pruner.patterns.get_pattern"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_pattern</span></code></a>(config)</p></td>
<td><p>Get registered pattern class.</p></td>
</tr>
</tbody>
</table>
<dl class="py function">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.register_pattern">
<span class="sig-prename descclassname"><span class="pre">neural_compressor.experimental.pytorch_pruner.patterns.</span></span><span class="sig-name descname"><span class="pre">register_pattern</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.register_pattern" title="Permalink to this definition">¶</a></dt>
<dd><p>Class decorator used to register a Pattern subclass to the registry.</p>
<p>Decorator function used before a Pattern subclasses.
Make sure that this Pattern class can be registered in PATTERNS.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cls</strong> (<em>class</em>) – The class of register.</p></li>
<li><p><strong>name</strong> – A string. Define the pattern type which will be included in a pruning process.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The class of register.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>cls</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.get_pattern">
<span class="sig-prename descclassname"><span class="pre">neural_compressor.experimental.pytorch_pruner.patterns.</span></span><span class="sig-name descname"><span class="pre">get_pattern</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.get_pattern" title="Permalink to this definition">¶</a></dt>
<dd><p>Get registered pattern class.</p>
<p>Get a Pattern object from PATTERNS.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config</strong> – A config dict object. Contains the pattern information.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A Pattern object.</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>AssertionError</strong> – Currently only support patterns which have been registered in PATTERNS.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.Pattern">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_compressor.experimental.pytorch_pruner.patterns.</span></span><span class="sig-name descname"><span class="pre">Pattern</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern" title="Permalink to this definition">¶</a></dt>
<dd><p>Pruning Pattern.</p>
<p>Every Pruner object will contain a Pattern object.
It defines the basic pruning unit and how this unit will be pruned during pruning.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config</strong> – A config dict object. Contains the pattern information.</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.Pattern.pattern">
<span class="sig-name descname"><span class="pre">pattern</span></span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.pattern" title="Permalink to this definition">¶</a></dt>
<dd><p>A config dict object. The pattern related part in args config.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.Pattern.is_global">
<span class="sig-name descname"><span class="pre">is_global</span></span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.is_global" title="Permalink to this definition">¶</a></dt>
<dd><p>A bool. Whether the pruning take global pruning option.
Global pruning means that all pruning layers are gathered to calculate pruning criteria.
Local pruning, on the contrast, means that pruning layers are to calculate criteria individually.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_masks">
<span class="sig-name descname"><span class="pre">get_masks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_sparsity_ratio</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_masks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_sparsity_ratio_per_layer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_masks" title="Permalink to this definition">¶</a></dt>
<dd><p>Call when new masks for pruning are to be calculated.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scores</strong> – A dict{“layer_name”: Tensor}. Store the pruning scores of weights.</p></li>
<li><p><strong>target_sparsity_ratio</strong> – A float. After pruning, the model’s sparsity will reach this value.</p></li>
<li><p><strong>pre_masks</strong> – A dict{“layer_name”: Tensor}. The masks generated after the last pruning step.</p></li>
<li><p><strong>max_sparsity_ratio_per_layer</strong> – A float. The maximum sparsity that one layer can reach.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dict with the identical size as pre_masks. Update the 0/1 values in it.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_masks_global">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">get_masks_global</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_sparsity_ratio</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_masks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_sparsity_ratio_per_layer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_masks_global" title="Permalink to this definition">¶</a></dt>
<dd><p>To be implemented in subclasses.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_mask_single">
<span class="sig-name descname"><span class="pre">get_mask_single</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">score</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exact_sparsity_ratio</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_mask_single" title="Permalink to this definition">¶</a></dt>
<dd><p>Obtain a mask for one layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>score</strong> – A Tensor. Store the pruning scores of one layer.</p></li>
<li><p><strong>exact_sparsity_ratio</strong> – A float. After pruning, the layer’s sparsity will reach this value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A Tensor with the identical size as score. a new mask.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_block_size_dict">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">get_block_size_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_block_size_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>To be implemented in subclasses.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_masks_local">
<span class="sig-name descname"><span class="pre">get_masks_local</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_sparsity_ratio</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_masks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_sparsity_ratio_per_layer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_masks_local" title="Permalink to this definition">¶</a></dt>
<dd><p>Obtain layers’ local masks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scores</strong> – A dict{“layer_name”: Tensor}. Store the pruning scores of weights.</p></li>
<li><p><strong>target_sparsity_ratio</strong> – A float. After pruning, the model’s sparsity will reach this value.</p></li>
<li><p><strong>pre_masks</strong> – A dict{“layer_name”: Tensor}. The masks generated after the last pruning step.</p></li>
<li><p><strong>max_sparsity_ratio_per_layer</strong> – A float. The maximum sparsity that one layer can reach.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dict with the identical size as pre_masks. Update the 0/1 values in it.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_sparsity_ratio">
<span class="sig-name descname"><span class="pre">get_sparsity_ratio</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pre_masks</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_sparsity_ratio" title="Permalink to this definition">¶</a></dt>
<dd><p>Calulate the zero elements’ ration in pre_masks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>pre_masks</strong> – Dict{“layer_name”: Tensor}. The masks generated after the last pruning step.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A float. The zero elements’ ratio in pre_masks.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_pattern_lock_masks">
<span class="sig-name descname"><span class="pre">get_pattern_lock_masks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modules</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_pattern_lock_masks" title="Permalink to this definition">¶</a></dt>
<dd><p>Obtain masks from original weight map, by masking where weights’ are zero.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>modules</strong> – A dict{“layer_name”: Tensor}. Store weights.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dict with the identical size as modules, containing pattern lock masks.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_compressor.experimental.pytorch_pruner.patterns.</span></span><span class="sig-name descname"><span class="pre">PatternNxM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern" title="neural_compressor.experimental.pytorch_pruner.patterns.Pattern"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Pattern</span></code></a></p>
<p>Pruning Pattern.</p>
<p>A Pattern class derived from Pattern. In this pattern, the weights in a NxM block will be pruned or kept
during one pruning step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config</strong> – A config dict object. Contains the pattern information.</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.block_size">
<span class="sig-name descname"><span class="pre">block_size</span></span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.block_size" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of two Integers. The height and width of the block.
Please be aware that the vertical direction of a Linear layer’s weight in PyTorch refer to output channel.
Because PyTorch’s tensor matmul has a hidden transpose operation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_block_size_dict">
<span class="sig-name descname"><span class="pre">get_block_size_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_block_size_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Calulate the zero elements’ ration in pre_masks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data</strong> – Dict{“layer_name”: Tensor}. Store weights or scores.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>[block_size_1, block_size_2]}.</dt><dd><p>Containing layers’ corresponding pruning pattern’s block shape.
Please be aware that because in channel-wise pruning,
different layers can have different pruning patterns.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict. Dict{“layer_name”</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_sparsity_ratio">
<span class="sig-name descname"><span class="pre">get_sparsity_ratio</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pre_masks</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_sparsity_ratio" title="Permalink to this definition">¶</a></dt>
<dd><p>Calulate the zero elements’ ration in pre_masks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>pre_masks</strong> – Dict{“layer_name”: Tensor}. The masks generated after the last pruning step.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A float. Calculate the zero elements’ ratio in pre_masks.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_masks_global">
<span class="sig-name descname"><span class="pre">get_masks_global</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_sparsity_ratio</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_masks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_sparsity_ratio_per_layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_pre_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_masks_global" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate masks for layers.</p>
<p>Gather all layer’s scores together and calculate a common threshold.
This threshold will be applied for all layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scores</strong> – A dict{“layer_name”: Tensor}. Store the pruning scores of weights.</p></li>
<li><p><strong>target_sparsity_ratio</strong> – A float. After pruning, the model’s sparsity will reach this value.</p></li>
<li><p><strong>pre_masks</strong> – A dict{“layer_name”: Tensor}. The masks generated after the last pruning step.</p></li>
<li><p><strong>max_sparsity_ratio_per_layer</strong> – A float. The maximum sparsity that one layer can reach.</p></li>
<li><p><strong>keep_pre_masks</strong> – A bool. If True, keep the masks unchanged.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dict with the identical size as pre_masks. Update the 0/1 values in it.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_pattern_lock_masks">
<span class="sig-name descname"><span class="pre">get_pattern_lock_masks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modules</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_pattern_lock_masks" title="Permalink to this definition">¶</a></dt>
<dd><p>Obtain masks from original weight map, by masking where weights’ are zero.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>modules</strong> – A dict{“layer_name”: Tensor}. Store weights.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dict with the identical size as modules, containing pattern lock masks.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_compressor.experimental.pytorch_pruner.patterns.</span></span><span class="sig-name descname"><span class="pre">PatternNInM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern" title="neural_compressor.experimental.pytorch_pruner.patterns.Pattern"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Pattern</span></code></a></p>
<p>Pruning Pattern.</p>
<p>A Pattern class derived from Pattern. In this pattern, N out of every M continuous weights will be pruned.
For more info of this pattern, please refer to
<a class="reference external" href="https://github.com/intel/neural-compressor/blob/master/docs/pruning.md">https://github.com/intel/neural-compressor/blob/master/docs/pruning.md</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config</strong> – A config dict object. Contains the pattern information.</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.N">
<span class="sig-name descname"><span class="pre">N</span></span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.N" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of elements to be prune in a weight sequence.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.M">
<span class="sig-name descname"><span class="pre">M</span></span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.M" title="Permalink to this definition">¶</a></dt>
<dd><p>The size of the weight sequence.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.get_sparsity_ratio">
<span class="sig-name descname"><span class="pre">get_sparsity_ratio</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pre_masks</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.get_sparsity_ratio" title="Permalink to this definition">¶</a></dt>
<dd><p>Calulate the zero elements’ ration in pre_masks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>pre_masks</strong> – Dict{“layer_name”: Tensor}. The masks generated after the last pruning step.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A float. Calculate the zero elements’ ratio in pre_masks.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.get_masks_global">
<span class="sig-name descname"><span class="pre">get_masks_global</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_sparsity_ratio</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_masks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_sparsity_ratio_per_layer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.get_masks_global" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate masks for layers.</p>
<p>Gather all layer’s scores together and calculate a common threshold.
This threshold will be applied for all layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scores</strong> – A dict{“layer_name”: Tensor}. Store the pruning scores of weights.</p></li>
<li><p><strong>target_sparsity_ratio</strong> – A float. After pruning, the model’s sparsity will reach this value.</p></li>
<li><p><strong>pre_masks</strong> – A dict{“layer_name”: Tensor}. The masks generated after the last pruning step.</p></li>
<li><p><strong>max_sparsity_ratio_per_layer</strong> – A float. The maximum sparsity that one layer can reach.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dict with the identical size as pre_masks. Update the 0/1 values in it.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.get_pattern_lock_masks">
<span class="sig-name descname"><span class="pre">get_pattern_lock_masks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modules</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.get_pattern_lock_masks" title="Permalink to this definition">¶</a></dt>
<dd><p>Obtain masks from original weight map, by masking where weights’ are zero.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>modules</strong> – A dict{“layer_name”: Tensor}. Store weights.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dict with the identical size as modules, containing pattern lock masks.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Intel® Neural Compressor.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#"><code class="xref py py-mod docutils literal notranslate"><span class="pre">neural_compressor.experimental.pytorch_pruner.patterns</span></code></a><ul>
<li><a class="reference internal" href="#module-contents">Module Contents</a><ul>
<li><a class="reference internal" href="#classes">Classes</a></li>
<li><a class="reference internal" href="#functions">Functions</a><ul>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.register_pattern"><code class="docutils literal notranslate"><span class="pre">register_pattern()</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.get_pattern"><code class="docutils literal notranslate"><span class="pre">get_pattern()</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern"><code class="docutils literal notranslate"><span class="pre">Pattern</span></code></a><ul>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.pattern"><code class="docutils literal notranslate"><span class="pre">Pattern.pattern</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.is_global"><code class="docutils literal notranslate"><span class="pre">Pattern.is_global</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_masks"><code class="docutils literal notranslate"><span class="pre">Pattern.get_masks()</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_masks_global"><code class="docutils literal notranslate"><span class="pre">Pattern.get_masks_global()</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_mask_single"><code class="docutils literal notranslate"><span class="pre">Pattern.get_mask_single()</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_block_size_dict"><code class="docutils literal notranslate"><span class="pre">Pattern.get_block_size_dict()</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_masks_local"><code class="docutils literal notranslate"><span class="pre">Pattern.get_masks_local()</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_sparsity_ratio"><code class="docutils literal notranslate"><span class="pre">Pattern.get_sparsity_ratio()</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_pattern_lock_masks"><code class="docutils literal notranslate"><span class="pre">Pattern.get_pattern_lock_masks()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM"><code class="docutils literal notranslate"><span class="pre">PatternNxM</span></code></a><ul>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.block_size"><code class="docutils literal notranslate"><span class="pre">PatternNxM.block_size</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_block_size_dict"><code class="docutils literal notranslate"><span class="pre">PatternNxM.get_block_size_dict()</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_sparsity_ratio"><code class="docutils literal notranslate"><span class="pre">PatternNxM.get_sparsity_ratio()</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_masks_global"><code class="docutils literal notranslate"><span class="pre">PatternNxM.get_masks_global()</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_pattern_lock_masks"><code class="docutils literal notranslate"><span class="pre">PatternNxM.get_pattern_lock_masks()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM"><code class="docutils literal notranslate"><span class="pre">PatternNInM</span></code></a><ul>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.N"><code class="docutils literal notranslate"><span class="pre">PatternNInM.N</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.M"><code class="docutils literal notranslate"><span class="pre">PatternNInM.M</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.get_sparsity_ratio"><code class="docutils literal notranslate"><span class="pre">PatternNInM.get_sparsity_ratio()</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.get_masks_global"><code class="docutils literal notranslate"><span class="pre">PatternNInM.get_masks_global()</span></code></a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.get_pattern_lock_masks"><code class="docutils literal notranslate"><span class="pre">PatternNInM.get_pattern_lock_masks()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.register_pattern">register_pattern</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.get_pattern">get_pattern</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern">Pattern</a><ul>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.pattern">pattern</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.is_global">is_global</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_masks">get_masks</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_masks_global">get_masks_global</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_mask_single">get_mask_single</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_block_size_dict">get_block_size_dict</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_masks_local">get_masks_local</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_sparsity_ratio">get_sparsity_ratio</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.Pattern.get_pattern_lock_masks">get_pattern_lock_masks</a></li>
</ul>
</li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM">PatternNxM</a><ul>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.block_size">block_size</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_block_size_dict">get_block_size_dict</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_sparsity_ratio">get_sparsity_ratio</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_masks_global">get_masks_global</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNxM.get_pattern_lock_masks">get_pattern_lock_masks</a></li>
</ul>
</li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM">PatternNInM</a><ul>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.N">N</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.M">M</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.get_sparsity_ratio">get_sparsity_ratio</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.get_masks_global">get_masks_global</a></li>
<li><a class="reference internal" href="#neural_compressor.experimental.pytorch_pruner.patterns.PatternNInM.get_pattern_lock_masks">get_pattern_lock_masks</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>
  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../../../',
            VERSION:'',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../../../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../../../../_static/sphinx_highlight.js"></script>

  

  <script type="text/javascript" src="../../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
<script script type="text/javascript">
  var collapsedSections = ['start', 'autoapi', 'info'];
</script>



  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://shiftlab.github.io/pytorch/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://shiftlab.github.io/pytorch/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://shiftlab.github.io/pytorch/">PyTorch</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/get-started">Get Started</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/features">Features</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/ecosystem">Ecosystem</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/blog/">Blog</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://shiftlab.github.io/pytorch/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://shiftlab.github.io/pytorch/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    mobileMenu.bind();
    mobileTOC.bind();
    pytorchAnchors.bind();

    $(window).on("load", function() {
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
    })

    // Add class to links that have code blocks, since we cannot create links in code blocks
    $("article.pytorch-article a span.pre").each(function(e) {
      $(this).closest("a").addClass("has-code");
    });
  </script>
</body>
</html>