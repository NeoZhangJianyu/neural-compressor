


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Metrics &mdash; Intel® Neural Compressor  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://shiftlab.github.io/pytorch/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://shiftlab.github.io/pytorch/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/features">Features</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

  
<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <div class="navbar-logo">
          <a href="https://intel.github.io/neural-compressor/Welcome.html" alt="Intel homepage" class="intel-logo-rebrand">
            <span class="headTitleStyle"> Intel® Neural Compressor</span> 
          </a>
        <div class="version">
              <a href="../versions.html">2.0▼</a>
              <p>Click link above to switch version</p>
            </div>
    </div>
      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  <div>

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
   

          </div>

          

            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="Welcome.html">Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">

      <section data-toggle="wy-nav-shift" class="pytorch-content-wrap">
        <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
          <div class="pytorch-breadcrumbs-wrapper">
            















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Metrics</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/metric.md.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
          </div>

          <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
            Shortcuts
          </div>
        </div>

        <div class="pytorch-content-left">
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" class="pytorch-article">
              
  <section id="metrics">
<h1>Metrics<a class="headerlink" href="#metrics" title="Permalink to this heading">¶</a></h1>
<ol>
<li><p><a class="reference external" href="#introduction">Introduction</a></p></li>
<li><p><a class="reference external" href="#supported-built-in-metric-matrix">Supported Built-in Metric Matrix</a></p>
<p>2.1. <a class="reference external" href="#tensorflow">TensorFlow</a></p>
<p>2.2. <a class="reference external" href="#pytorch">PyTorch</a></p>
<p>2.3. <a class="reference external" href="#mxnet">MxNet</a></p>
<p>2.4. <a class="reference external" href="#onnxrt">ONNXRT</a></p>
</li>
<li><p><a class="reference external" href="#get-start-with-metrics">Get Start with Metrics</a></p>
<p>3.1. <a class="reference external" href="#support-single-metric-and-multi-metrics">Support Single-metric and Multi-metrics</a></p>
<p>3.2. <a class="reference external" href="#build-custom-metric-with-python-api">Build Custom Metric with Python API</a></p>
</li>
</ol>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h2>
<p>In terms of evaluating the performance of a specific model, we should have general metrics to measure the performance of different models. Different frameworks always have their own Metric module but with different APIs and parameters. Neural Compressor Metrics supports code-free configuration through a yaml file, with built-in metrics, so that Neural Compressor can achieve performance and accuracy without code changes from the user. In special cases, users can also register their own metric classes through <a class="reference external" href="#build-custom-metric-in-code">building custom metric in code</a>.</p>
</section>
<section id="supported-built-in-metric-matrix">
<h2>Supported Built-in Metric Matrix<a class="headerlink" href="#supported-built-in-metric-matrix" title="Permalink to this heading">¶</a></h2>
<p>Neural Compressor supports some built-in metrics that are popularly used in industry.</p>
<section id="tensorflow">
<h3>TensorFlow<a class="headerlink" href="#tensorflow" title="Permalink to this heading">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Parameters</th>
<th style="text-align: left;">Inputs</th>
<th style="text-align: left;">Comments</th>
<th style="text-align: left;">Usage(In yaml file)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">topk(k)</td>
<td style="text-align: left;"><strong>k</strong> (int, default=1): Number of top elements to look at for computing accuracy</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes top k predictions accuracy.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; topk: <br> &ensp;&ensp;&ensp;&ensp; k: 1</td>
</tr>
<tr>
<td style="text-align: left;">Accuracy()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes accuracy classification score.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; Accuracy: {}</td>
</tr>
<tr>
<td style="text-align: left;">Loss()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">A dummy metric for directly printing loss, it calculates the average of predictions. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/_modules/mxnet/metric.html#Loss">MXNet docs</a> for details.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; Loss: {}</td>
</tr>
<tr>
<td style="text-align: left;">MAE(compare_label)</td>
<td style="text-align: left;"><strong>compare_label</strong> (bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes Mean Absolute Error (MAE) loss.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; MAE: {}</td>
</tr>
<tr>
<td style="text-align: left;">RMSE(compare_label)</td>
<td style="text-align: left;"><strong>compare_label</strong> (bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes Root Mean Square Error (RMSE) loss.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; RMSE: {}</td>
</tr>
<tr>
<td style="text-align: left;">MSE(compare_label)</td>
<td style="text-align: left;"><strong>compare_label</strong> (bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes Mean Squared Error (MSE) loss.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; MSE: {}</td>
</tr>
<tr>
<td style="text-align: left;">F1()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes the F1 score of a binary classification problem.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; F1: {}</td>
</tr>
<tr>
<td style="text-align: left;">mAP(anno_path, iou_thrs, map_points)</td>
<td style="text-align: left;"><strong>anno_path</strong> (str): Annotation path. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/tensorflow_models/quantization/ptq/label_map.yaml">label_map</a> for its format. <br> <strong>iou_thrs</strong> (float or str, default=0.5): Minimal value for intersection over union that allows to make decision that prediction bounding box is true positive. You can specify one float value between 0 to 1 or string "05:0.05:0.95" for standard COCO thresholds.<br> <strong>map_points</strong> (int, default=0): The way to calculate mAP. 101 for 101-point interpolated AP, 11 for 11-point interpolated AP, 0 for area under PR curve.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; mAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br> &ensp;&ensp;&ensp;&ensp; iou_thrs: 0.5 <br> &ensp;&ensp;&ensp;&ensp; map_points: 0 <br><br> If anno_path is not set, metric will use official coco label id</td>
</tr>
<tr>
<td style="text-align: left;">COCOmAP(anno_path, iou_thrs, map_points)</td>
<td style="text-align: left;"><strong>anno_path</strong> (str): Annotation path. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/tensorflow_models/quantization/ptq/label_map.yaml">label_map</a> for its format. <br> <strong>iou_thrs</strong> (float or str): Intersection over union threshold. Set to "0.5:0.05:0.95" for standard COCO thresholds.<br> <strong>map_points</strong> (int): The way to calculate mAP. Set to 101 for 101-point interpolated AP.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; COCOmAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br><br> If anno_path is not set, metric will use official coco label id</td>
</tr>
<tr>
<td style="text-align: left;">VOCmAP(anno_path, iou_thrs, map_points)</td>
<td style="text-align: left;"><strong>anno_path</strong>(str): Annotation path. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/tensorflow_models/quantization/ptq/label_map.yaml">label_map</a> for its format. <br> <strong>iou_thrs</strong>(float or str): Intersection over union threshold. Set to 0.5.<br> <strong>map_points</strong>(int): The way to calculate mAP. The way to calculate mAP. Set to 0 for area under PR curve.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; VOCmAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br><br> If anno_path is not set, metric will use official coco label id</td>
</tr>
<tr>
<td style="text-align: left;">COCOmAPv2(anno_path, iou_thrs, map_points, output_index_mapping)</td>
<td style="text-align: left;"><strong>anno_path</strong> (str): Annotation path. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/tensorflow_models/quantization/ptq/label_map.yaml">label_map</a> for its format. <br><strong>iou_thrs</strong> (float or str): Intersection over union threshold. Set to "0.5:0.05:0.95" for standard COCO thresholds.<br> <strong>map_points</strong> (int): The way to calculate mAP. Set to 101 for 101-point interpolated AP. <br> <strong>output_index_mapping</strong>(dict, default={'num_detections':-1, 'boxes':0, 'scores':1, 'classes':2}): Specifies the index of outputs in model raw prediction, -1 means this output does not exist.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; COCOmAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br> &ensp;&ensp;&ensp;&ensp; output_index_mapping: <br> &ensp;&ensp;&ensp;&ensp;&ensp;&ensp; num_detections: 0 <br> &ensp;&ensp;&ensp;&ensp;&ensp;&ensp; boxes: 1 <br> &ensp;&ensp;&ensp;&ensp;&ensp;&ensp; scores: 2 <br> &ensp;&ensp;&ensp;&ensp;&ensp;&ensp; classes: 3 <br><br> If anno_path is not set, metric will use official coco label id</td>
</tr>
<tr>
<td style="text-align: left;">BLEU()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">BLEU score computation between labels and predictions. An approximate BLEU scoring method since we do not glue word pieces or decode the ids and tokenize the output. By default, we use ngram order of 4 and use brevity penalty. Also, this does not have beam search</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; BLEU: {}</td>
</tr>
<tr>
<td style="text-align: left;">SquadF1()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Evaluate v1.1 of the SQuAD dataset</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; SquadF1: {}</td>
</tr>
</tbody>
</table></section>
<section id="pytorch">
<h3>PyTorch<a class="headerlink" href="#pytorch" title="Permalink to this heading">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Parameters</th>
<th style="text-align: left;">Inputs</th>
<th style="text-align: left;">Comments</th>
<th style="text-align: left;">Usage(In yaml file)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">topk(k)</td>
<td style="text-align: left;"><strong>k</strong> (int, default=1): Number of top elements to look at for computing accuracy</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Calculates the top-k categorical accuracy.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; topk: <br> &ensp;&ensp;&ensp;&ensp; k: 1</td>
</tr>
<tr>
<td style="text-align: left;">Accuracy()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Calculates the accuracy for binary, multiclass and multilabel data. <br> Please refer <a href="https://pytorch.org/ignite/metrics.html#ignite.metrics.Accuracy">Pytorch docs</a> for details.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; Accuracy: {}</td>
</tr>
<tr>
<td style="text-align: left;">Loss()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">A dummy metric for directly printing loss, it calculates the average of predictions. <br> Please refer <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/_modules/mxnet/metric.html#Loss">MXNet docs</a> for details.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; Loss: {}</td>
</tr>
<tr>
<td style="text-align: left;">MAE(compare_label)</td>
<td style="text-align: left;"><strong>compare_label</strong>(bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes Mean Absolute Error (MAE) loss.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; MAE: <br> &ensp;&ensp;&ensp;&ensp; compare_label： True</td>
</tr>
<tr>
<td style="text-align: left;">RMSE(compare_label)</td>
<td style="text-align: left;"><strong>compare_label</strong>(bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes Root Mean Squared Error (RMSE) loss.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; RMSE: <br> &ensp;&ensp;&ensp;&ensp; compare_label: True</td>
</tr>
<tr>
<td style="text-align: left;">MSE(compare_label)</td>
<td style="text-align: left;"><strong>compare_label</strong>(bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes Mean Squared Error (MSE) loss.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; MSE: <br> &ensp;&ensp;&ensp;&ensp; compare_label: True</td>
</tr>
<tr>
<td style="text-align: left;">F1()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes the F1 score of a binary classification problem.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; F1: {}</td>
</tr>
</tbody>
</table></section>
<section id="mxnet">
<h3>MXNet<a class="headerlink" href="#mxnet" title="Permalink to this heading">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Parameters</th>
<th style="text-align: left;">Inputs</th>
<th style="text-align: left;">Comments</th>
<th style="text-align: left;">Usage(In yaml file)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">topk(k)</td>
<td style="text-align: left;"><strong>k</strong> (int, default=1): Number of top elements to look at for computing accuracy</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes top k predictions accuracy.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; topk: <br> &ensp;&ensp;&ensp;&ensp; k: 1</td>
</tr>
<tr>
<td style="text-align: left;">Accuracy()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes accuracy classification score. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/metric/index.html#mxnet.metric.Accuracy">MXNet docs</a> for details.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; Accuracy: {}</td>
</tr>
<tr>
<td style="text-align: left;">Loss()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">A dummy metric for directly printing loss, it calculates the average of predictions. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/_modules/mxnet/metric.html#Loss">MXNet docs</a> for details.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; Loss: {}</td>
</tr>
<tr>
<td style="text-align: left;">MAE()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes Mean Absolute Error (MAE) loss. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/metric/index.html#mxnet.metric.MAE">MXNet docs</a> for details.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; MAE: {}</td>
</tr>
<tr>
<td style="text-align: left;">RMSE(compare_label)</td>
<td style="text-align: left;"><strong>compare_label</strong>(bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes Root Mean Squared Error (RMSE) loss.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; RMSE: <br> &ensp;&ensp;&ensp;&ensp; compare_label: True</td>
</tr>
<tr>
<td style="text-align: left;">MSE()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes Mean Squared Error (MSE) loss. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/metric/index.html#mxnet.metric.MSE">MXNet docs</a> for details.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; MSE: {}</td>
</tr>
<tr>
<td style="text-align: left;">F1()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes the F1 score of a binary classification problem. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/metric/index.html#mxnet.metric.F1">MXNet docs</a> for details.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; F1: {}</td>
</tr>
</tbody>
</table></section>
<section id="onnxrt">
<h3>ONNXRT<a class="headerlink" href="#onnxrt" title="Permalink to this heading">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Parameters</th>
<th style="text-align: left;">Inputs</th>
<th style="text-align: left;">Comments</th>
<th style="text-align: left;">Usage(In yaml file)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">topk(k)</td>
<td style="text-align: left;"><strong>k</strong> (int, default=1): Number of top elements to look at for computing accuracy</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes top k predictions accuracy.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; topk: <br> &ensp;&ensp;&ensp;&ensp; k: 1</td>
</tr>
<tr>
<td style="text-align: left;">Accuracy()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes accuracy classification score.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; Accuracy: {}</td>
</tr>
<tr>
<td style="text-align: left;">Loss()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">A dummy metric for directly printing loss, it calculates the average of predictions. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/_modules/mxnet/metric.html#Loss">MXNet docs</a> for details.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; Loss: {}</td>
</tr>
<tr>
<td style="text-align: left;">MAE(compare_label)</td>
<td style="text-align: left;"><strong>compare_label</strong>(bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes Mean Absolute Error (MAE) loss.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; MAE: <br> &ensp;&ensp;&ensp;&ensp; compare_label： True</td>
</tr>
<tr>
<td style="text-align: left;">RMSE(compare_label)</td>
<td style="text-align: left;"><strong>compare_label</strong>(bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes Root Mean Squared Error (RMSE) loss.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; RMSE: <br> &ensp;&ensp;&ensp;&ensp; compare_label: True</td>
</tr>
<tr>
<td style="text-align: left;">MSE(compare_label)</td>
<td style="text-align: left;"><strong>compare_label</strong>(bool, default=True): Whether to compare label. False if there are no labels and will use FP32 preds as labels.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes Mean Squared Error (MSE) loss.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; MSE: <br> &ensp;&ensp;&ensp;&ensp; compare_label: True</td>
</tr>
<tr>
<td style="text-align: left;">F1()</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes the F1 score of a binary classification problem.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; F1: {}</td>
</tr>
<tr>
<td style="text-align: left;">mAP(anno_path, iou_thrs, map_points)</td>
<td style="text-align: left;"><strong>anno_path</strong>(str): Annotation path. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/tensorflow_models/quantization/ptq/label_map.yaml">label_map</a> for its format. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/tensorflow_models/quantization/ptq/label_map.yaml">label_map</a> for its format. <br> <strong>iou_thrs</strong>(float or str, default=0.5): Minimal value for intersection over union that allows to make decision that prediction bounding box is true positive. You can specify one float value between 0 to 1 or string "05:0.05:0.95" for standard COCO thresholds.<br> <strong>map_points</strong>(int, default=0): The way to calculate mAP. 101 for 101-point interpolated AP, 11 for 11-point interpolated AP, 0 for area under PR curve.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; mAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br> &ensp;&ensp;&ensp;&ensp; iou_thrs: 0.5 <br> &ensp;&ensp;&ensp;&ensp; map_points: 0 <br><br> If anno_path is not set, metric will use official coco label id</td>
</tr>
<tr>
<td style="text-align: left;">COCOmAP(anno_path, iou_thrs, map_points)</td>
<td style="text-align: left;"><strong>anno_path</strong>(str): Annotation path. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/tensorflow_models/quantization/ptq/label_map.yaml">label_map</a> for its format. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/tensorflow_models/quantization/ptq/label_map.yaml">label_map</a> for its format. <br> <strong>iou_thrs</strong>(float or str): Intersection over union threshold. Set to "0.5:0.05:0.95" for standard COCO thresholds.<br> <strong>map_points</strong>(int): The way to calculate mAP. Set to 101 for 101-point interpolated AP.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; COCOmAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br><br> If anno_path is not set, metric will use official coco label id</td>
</tr>
<tr>
<td style="text-align: left;">VOCmAP(anno_path, iou_thrs, map_points)</td>
<td style="text-align: left;"><strong>anno_path</strong>(str): Annotation path. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/tensorflow_models/quantization/ptq/label_map.yaml">label_map</a> for its format. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/tensorflow_models/quantization/ptq/label_map.yaml">label_map</a> for its format .<br> <strong>iou_thrs</strong>(float or str): Intersection over union threshold. Set to 0.5.<br> <strong>map_points</strong>(int): The way to calculate mAP. The way to calculate mAP. Set to 0 for area under PR curve.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; VOCmAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br><br> If anno_path is not set, metric will use official coco label id</td>
</tr>
<tr>
<td style="text-align: left;">COCOmAPv2(anno_path, iou_thrs, map_points, output_index_mapping)</td>
<td style="text-align: left;"><strong>anno_path</strong>(str): Annotation path. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/tensorflow_models/quantization/ptq/label_map.yaml">label_map</a> for its format. The annotation file should be a yaml file, please refer to <a href="../examples/tensorflow/object_detection/tensorflow_models/quantization/ptq/label_map.yaml">label_map</a> for its format. <br><strong>iou_thrs</strong>(float or str): Intersection over union threshold. Set to "0.5:0.05:0.95" for standard COCO thresholds.<br> <strong>map_points</strong>(int): The way to calculate mAP. Set to 101 for 101-point interpolated AP.<br> <strong>output_index_mapping</strong>(dict, default={'num_detections':-1, 'boxes':0, 'scores':1, 'classes':2}): Specifies the index of outputs in model raw prediction, -1 means this output does not exist.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; COCOmAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br> &ensp;&ensp;&ensp;&ensp; output_index_mapping: <br> &ensp;&ensp;&ensp;&ensp;&ensp;&ensp; num_detections: 0 <br> &ensp;&ensp;&ensp;&ensp;&ensp;&ensp; boxes: 1 <br> &ensp;&ensp;&ensp;&ensp;&ensp;&ensp; scores: 2 <br> &ensp;&ensp;&ensp;&ensp;&ensp;&ensp; classes: 3<br><br> If anno_path is not set, metric will use official coco label id</td>
</tr>
<tr>
<td style="text-align: left;">GLUE(task)</td>
<td style="text-align: left;"><strong>task</strong> (str, default=mrpc): The name of the task. Choices include mrpc, qqp, qnli, rte, sts-b, cola, mnli, wnli.</td>
<td style="text-align: left;">preds, labels</td>
<td style="text-align: left;">Computes GLUE score for bert model.</td>
<td style="text-align: left;">metric: <br> &ensp;&ensp; GLUE: <br> &ensp;&ensp;&ensp;&ensp; task: mrpc</td>
</tr>
</tbody>
</table></section>
</section>
<section id="get-start-with-metrics">
<h2>Get Start with Metrics<a class="headerlink" href="#get-start-with-metrics" title="Permalink to this heading">¶</a></h2>
<section id="support-single-metric-and-multi-metrics">
<h3>Support Single-metric and Multi-metrics<a class="headerlink" href="#support-single-metric-and-multi-metrics" title="Permalink to this heading">¶</a></h3>
<p>Users can specify an Neural Compressor built-in metric such as shown below:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">evaluation</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">accuracy</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">metric</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">topk</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
</pre></div>
</div>
<p>In some cases, users want to use more than one metric to evaluate the performance of a specific model and they can realize it with multi_metrics of Neural Compressor. Currently multi_metrics supports built-in metrics.</p>
<p>There are two usages for multi_metrics of Neural Compressor:</p>
<ol class="simple">
<li><p>Evaluate performance of a model with metrics one by one</p></li>
</ol>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">evaluation</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">accuracy</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">multi_metrics</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">topk</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">      </span><span class="nt">MSE</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">compare_label</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span><span class="w"></span>
<span class="w">      </span><span class="nt">higher_is_better</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">True</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">False</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># length of higher_is_better should be equal to num of metric, default is True</span><span class="w"></span>
</pre></div>
</div>
<ol class="simple">
<li><p>Evaluate performance of a model with weighted metric results</p></li>
</ol>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">evaluation</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">accuracy</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">multi_metrics</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">topk</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">      </span><span class="nt">MSE</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">compare_label</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span><span class="w"></span>
<span class="w">      </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">0.5</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">0.5</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># length of weight should be equal to num of metric</span><span class="w"></span>
<span class="w">      </span><span class="nt">higher_is_better</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">True</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">False</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># length of higher_is_better should be equal to num of metric, default is True</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="build-custom-metric-with-python-api">
<h3>Build Custom Metric with Python API<a class="headerlink" href="#build-custom-metric-with-python-api" title="Permalink to this heading">¶</a></h3>
<p>Please refer to <a class="reference external" href="https://github.com/intel/neural-compressor/tree/7a39c862bf37a7b890824621dafedef8b4161c33/../neural_compressor/experimental/metric">Metrics code</a>, users can also register their own metric as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NewMetric</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># init code here</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="c1"># add preds and labels to storage</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># clear preds and labels storage</span>

    <span class="k">def</span> <span class="nf">result</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># calculate accuracy</span>
        <span class="k">return</span> <span class="n">accuracy</span>
</pre></div>
</div>
<p>The result() function returns a higher-is-better scalar to reflect model accuracy on an evaluation dataset.</p>
<p>After defining the metric class, users can initialize it and pass it to quantizer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="kn">from</span> <span class="nn">neural_compressor.quantization</span> <span class="kn">import</span> <span class="n">Quantization</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantization</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">graph</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">metric</span> <span class="o">=</span> <span class="n">NewMetric</span><span class="p">()</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">calib_dataloader</span> <span class="o">=</span> <span class="n">dataloader</span>
<span class="n">q_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Intel® Neural Compressor.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Metrics</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#supported-built-in-metric-matrix">Supported Built-in Metric Matrix</a><ul>
<li><a class="reference internal" href="#tensorflow">TensorFlow</a></li>
<li><a class="reference internal" href="#pytorch">PyTorch</a></li>
<li><a class="reference internal" href="#mxnet">MXNet</a></li>
<li><a class="reference internal" href="#onnxrt">ONNXRT</a></li>
</ul>
</li>
<li><a class="reference internal" href="#get-start-with-metrics">Get Start with Metrics</a><ul>
<li><a class="reference internal" href="#support-single-metric-and-multi-metrics">Support Single-metric and Multi-metrics</a></li>
<li><a class="reference internal" href="#build-custom-metric-with-python-api">Build Custom Metric with Python API</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>
  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/documentation_options.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="_static/sphinx_highlight.js"></script>

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
<script script type="text/javascript">
  var collapsedSections = ['start', 'autoapi', 'info'];
</script>



  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://shiftlab.github.io/pytorch/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://shiftlab.github.io/pytorch/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://shiftlab.github.io/pytorch/">PyTorch</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/get-started">Get Started</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/features">Features</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/ecosystem">Ecosystem</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/blog/">Blog</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://shiftlab.github.io/pytorch/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://shiftlab.github.io/pytorch/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    mobileMenu.bind();
    mobileTOC.bind();
    pytorchAnchors.bind();

    $(window).on("load", function() {
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
    })

    // Add class to links that have code blocks, since we cannot create links in code blocks
    $("article.pytorch-article a span.pre").each(function(e) {
      $(this).closest("a").addClass("has-code");
    });
  </script>
</body>
</html>